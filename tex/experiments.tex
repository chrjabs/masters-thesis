\chapter{Experiments\label{chap:experiments}}

% Experiment setup and some signposting
We implemented  all variants and refinements of \algname{} described in \cref{chap:approach} in C++.
Our implementations of MSU3 and OLL were inspired by their implementations in Open-WBO v2.1~\autocite{DBLP:conf/sat/MartinsML14}, the other variants were implemented from scratch.
We used the state-of-the-art~\autocite{DBLP:journals/ai/FroleyksHIJS21} SAT solver CaDiCaL v1.5.2~\autocite{BiereFazekasFleuryHeisinger-SAT-Competition-2020-solvers}.
As three competing approaches we chose $P$-minimal, ParetoMCS and Seesaw.
Our implementation, is available as open source ({\small\url{https://bitbucket.org/coreo-group/bioptsat/}}).
We also implemented $P$-minimal and Seesaw from scratch as, to the best of our knowledge, no general implementation of these approaches are publicly available.

In our implementation, the core-guided variants of \algname{} use heuristic core minimization by default and dominated solutions are not blocked in \satunsat{}.
As an additional parametric detail, in its default \msh{} is configured to switch between \msu{} and \satunsat{} once 70\% of the literals in $\Obj_\inc$ have been added to $\tot(\Act)$.

We empirically evaluate the relative runtime performance of the \algname{} variants against the three competing approaches, as well as the impact of the specific refinements (recall \cref{sec:refinements}) to \algname{} on their runtime performance.
This comparison is done for the two tasks of finding a single representative solution per Pareto point and enumerating all Pareto-optimal solutions.
We also investigate how much CPU time is spent in the \Min{} compared to the \Simpr{} subroutines of \algname{}.

All experiments were run on 2.60-GHz Intel Xeon E5-2670 machines with 64-GB RAM in RHEL under a 1.5-hour per-instance time and 16-GB memory limit.

\section{Benchmarks\label{sec:benchmarks}}

% Signposting
We empirically evaluate the performance of \algname{} and the competing approaches on two bi-objective optimization problems:
learning interpretable decision rules from data and bi-objective set covering.

\subsection{Learning Interpretable Decision Rules\label{sec:lidr}}

% What is MLIC
Recently, a variety of SAT and MaxSAT-based approaches have been developed for learning interpretable classifiers from data~\autocites{DBLP:conf/ijcai/Ignatiev0NS21,DBLP:conf/cp/MaliotovM18,DBLP:conf/ijcai/NarodytskaIPM18,DBLP:conf/ijcai/Hu0HH20,DBLP:journals/corr/abs-2010-09919,DBLP:conf/cp/YuISB20,DBLP:conf/cade/IgnatievPNM18}.
The two objectives of minimizing size (the smaller, the more interpretable) and classification error (when there is no perfect classifier, as typical for real-world data) are conflicting, hence naturally giving rise to bi-objective optimization problems.
Here we consider learning of interpretable decision rules as a representative benchmark domain from this line of work, building on the encoding presented by~\textcite{DBLP:conf/cp/MaliotovM18}.
In short, a decision rule is a binary classifier in the form of a CNF formula over Boolean features.
The result of the formula evaluated on the features of a data sample is the binary classification assigned by the classifier to this sample.
Since MaxSAT only allows for a single objective, in~\autocite{DBLP:conf/cp/MaliotovM18} a linear combination of the two objectives, using a parameter $\lambda\geq 0$, was proposed.
This is equivalent to the so-called weighted sum method~\autocite{Ehrgott2005-3}.
While this allows for finding a Pareto-optimal decision rule under a specific value of $\lambda$, MaxSAT solving multiple times under different choices of $\lambda$ does not guarantee finding a representative Pareto-optimal decision rule for each Pareto point~\autocites{Ehrgott2005-3,survey}.
In contrast, here we address directly the problem of computing all Pareto-optimal solutions w.r.t.\ the two objectives.
\bigskip

% Example: Decision rule
\begin{minipage}{.75\textwidth}
  \begin{example}\label{ex:dr}
    Consider the sample dataset shown on the right with features $x_1$ and $x_2$, class label $y$ and three samples.
    Two example decision rules are $r_1 = (x_1)$, $r_2 = (x_1 \land x_2)$.
    Rule $r_1$ has size 1 and classification error 1, while $r_2$ has size 2 and classification error 0.
    Both, $r_1$ and $r_2$, are Pareto-optimal w.r.t\ size and classification error.
  \end{example}
\end{minipage}
\;
\begin{minipage}{.2\textwidth}
  \begin{center}
    \begin{tabular}{cc@{\hspace{2em}}c}
      \toprule
      $x_1$ & $x_2$ & $y$ \\
      \midrule
      1 & 1 & 1 \\
      0 & 1 & 0 \\
      1 & 0 & 0 \\
      \bottomrule
    \end{tabular}
  \end{center}
\end{minipage}
\bigskip

% The MLIC encoding and how we use it
For a given set of $\nsamp$ data samples over $\nfeat$ features, the encoding uses two sets of variables:
$\selector_l^j$ for $l=1,\dots,\nclauses$, $j=1,\dots,\nfeat$ and $\noise_i$ for $i=1,\dots,\nsamp$ for a specific number $\nclauses$ of clauses in the decision rules to be learned.
The interpretation of the variables is that $\selector_l^j=1$ if and only if the $j$th feature is included in the $l$th clause of the decision rule, and $\noise_i=1$ if the $i$th data sample is misclassified.
We represent the sample with index $i$ with a Boolean class label $y_i$ and the Boolean features $x_i^j$ where $j=1,\dots,\nfeat$.
With this, the encoding is
\[ \lnot \noise_i \rightarrow (y_i \leftrightarrow \bigwedge_{l=1}^\nclauses \bigvee_{j=1}^\nfeat (x_i^j \land \selector_l^j)). \]
We use this encoding as $\formula$, literals $\selector_l^j$ as $\Obj_\inc$ and literals $\noise_i$ as $\Obj_\dec$.
The task in this benchmark is therefore to find Pareto-optimal solution w.r.t.\ the size of the decision rule (measured as the number of literals in the rule) and its classification error.
In preliminary experiments, we found that choosing the objectives as described here leads to better performance than using classification error as the increasing objective.
This might be because the cores that can be extracted from the size objective are more beneficial for the search procedure.

% Symmetry breaking clauses
Since decision rules in CNF contain many symmetric solutions obtained by changing the order of clauses in the rule, we add additional clauses to the encoding to break these symmetries.
The idea behind the symmetry breaking is that the bit-strings $\sol(\selector_l^1)\sol(\selector_l^2)\dots\sol(\selector_l^\nfeat)$ are forced to be in lexicographic ordering.
In more detail, additionally to the $\selector$ variables, we introduce variables $\equals_l^j$ for $j=1,\dots,\nfeat$ and $l=2,\dots,\nclauses$ that represent whether the bit-strings of the clauses with index $(l-1)$ and $l$ are equal for the first $j$ bits.
The semantics of this representation are encoded as follows:
\begin{align*}
  &\equals_l^1 \leftrightarrow (\selector_{l-1}^1 \leftrightarrow \selector_l^1),\ \text{and} \\
  &\equals_l^j \leftrightarrow (\equals_l^{j-1} \land (\selector_{l-1}^f \leftrightarrow \selector_l^j))\ \text{for}\ j=2,\dots,\nfeat.
\end{align*}
The lexicographic ordering is then enforced by adding the constraints $\lnot \equals_l^1 \rightarrow (\selector_{l-1}^1 \land \lnot \selector_{l-1}^1)$ and $(\equals_l^{j-1} \land \lnot \equals_l^j) \rightarrow (\selector_{l-1}^j \land \lnot \selector_l^j)$ for $j=2,\dots,\nfeat$, enforcing that the bit with the smallest index in which the clauses differ should be 1 in the clause with index $(l-1)$ and 0 in the clause with index $l$.

% Example: Symmetric decision rules
\begin{example}
  Consider the rule $r_2 = (x_1 \land x_2)$ for the data in \cref{ex:dr}.
  It consists of two clauses, $C_1 = x_1$ and $C_2 = x_2$.
  In a solution $\sol$ to the encoding, $C_1$ will be represented as the bit-string $\sol(\selector_l^1)\sol(\selector_l^2)=10$ and $C_2$ as $\sol(\selector_l^1)\sol(\selector_l^2)=01$.
  Without symmetry breaking, either $\sol_1 = \{\selector_1^1, \lnot\selector_1^2, \lnot\selector_2^1, \selector_2^2\}$ or $\sol_2 = \{\lnot\selector_1^1, \selector_1^2, \selector_2^1, \lnot\selector_2^2\}$ would be valid solutions, even though they both map to $r_2$.
  The symmetry breaking clauses enforce that the bit-string representing $C_1$ precedes the bit-string representing $C_2$, therefore only $\sol_1$ is a feasible solution.
\end{example}

% Benchmark datasets
As the basis of our benchmark instances, we used 24 standard UCI~\autocite{UciMlr} and Kaggle ({\small\url{https://www.kaggle.com}}) benchmark datasets, including ones used in the original evaluation of the encoding~\autocite{DBLP:conf/cp/MaliotovM18}; see \cref{appendix:datasets} for details.
We independently at random sampled subsets of $\nsamp\in\{50,100,1000,5000,10000\}$ data samples from the datasets, four of each size (when applicable), resulting in a total of 372 datasets.
The datasets were discretized as in~\autocite{DBLP:conf/cp/MaliotovM18}:
categorical features are one-hot encoded, continuous features discretized by comparing to a collection of thresholds.
All experiments on these datasets were run with the encoding configured to learn CNF decision rules consisting of two clauses ($\nclauses = 2$).

% Domain-specific blocking
When enumerating multiple solutions corresponding to the same Pareto point, the blocking clauses for \algname{} (as well as $P$-minimal compared to in the experiments) can be strengthened to find solutions mapping to distinct rules:
blocking over the variables $s_l^j$ is sufficient and blocks multiple symmetric solutions that only differ in the assignment to auxiliary variables.
Further, making use of the algorithm-specific fact that \algname{} is guaranteed to enumerate Pareto-optimal solutions in order of increasing size, for \algname{} it is sufficient to block a solution over all $s_l^j$ that are assigned to false.

\subsection{Bi-Objective Set Covering}

% Bi-objective set covering and its encoding
In the set covering problem over sets $\sets$, a subset $\cover$ of the elements $\{1,\dots,\nelems\}$ needs to be chosen such that (i)~$\cover$ covers all sets in $\sets$, i.e., $\cover\cap S\neq\emptyset,\,\forall S\in\sets$, and (ii)~$\cover$ is minimal regarding some objective.
The bi-objective variant assigns each element $\element$ two different cost values $\cost^\element_1$ and $\cost^\element_2$ where the two different objectives for the cover $\cover$ are to minimize $c^\cover_1 = \sum_{\element\in\cover}\cost^\element_1$ and $c^\cover_2 = \sum_{\element\in\cover}\cost^\element_2$.
When encoding set covering into propositional logic, every set $S\in\sets$ forms one clause in the encoding, i.e., the clauses are $\{l_\element \mid \element\in S\}$ with $l_\element$ being a literal representing if element $\element$ is in $\cover$.
Furthermore, the integer values for the cost associated with element $\element$ can be represented by adding $l_\element$ to the objective set $\cost^\element$ times.
Note that multi-objective set covering was also used originally for evaluating $P$-minimal~\autocite{DBLP:conf/cp/SohBTB17}.

% Example: Set covering
\begin{example}
  Consider the sets $\sets = \{ \{a, b\}, \{b, d\} \}$ and costs $c^a_1 = c^b_1 = c^d_1 = c^a_2 = c^d_2 = 1$ and $c^b_2 = 5$.
  The two covers $\cover_1 = \{ b \}$ and $\cover_2 = \{ a, b \}$ are Pareto-optimal with $\cover_1$ having costs $c^{\cover_1}_1 = 1$ and $c^{\cover_1}_2 = 5$ and $\cover_2$ costs $c^{\cover_2}_1 = 2$ and $c^{\cover_2}_2 = 2$.
\end{example}

% Benchmark instances
We generated two types of  bi-objective set covering problem instances:
\begin{enumerate}[label=(\roman*)]
  \item using a fixed probability $\elemprob$ for an element appearing in a set (\scep{})
  \item using fixed set cardinality $\setcard$, with elements in a set chosen uniformly at random without replacement (\scsc{}).
\end{enumerate}
We generated both types of instances using combinations of the following parameters:
number of elements $\nelems\in\{100,150,200\}$, number of sets $\nsets\in\{20,40,60,80\}$, element probability $\elemprob\in\{0.1,0.2\}$ and set cardinality $\setcard\in\{5,10\}$.
For each combination, we generated five instances, leading to 120 instances of each type.
The integer cost values $\cost$ for the two objectives were chosen uniformly at random from the range $\cost\in[1,100]$.

% Domain-specific blocking
The blocking clauses used in \algname{} for enumerating all Pareto-optimal solutions can be strengthened also for set covering:
due to the fact that \algname{} is guaranteed to enumerate the Pareto-optimal solutions so that one of the objectives will monotonically decrease, in \algname{} it is enough to block the solution over all $l_e$ that are assigned to true.

\section{Competing Approaches\label{sec:competing}}

% Competitors and their implementations
We compare \algname{} to the three algorithms overviewed in \cref{sec:sat-based}: $P$-minimal, Seesaw and ParetoMCS.
These algorithms were chosen because they are similar in that they are (or, in the case of Seesaw, can be) SAT-based, and they solve the same task of enumerating Pareto-optimal solutions.
We implemented $P$-minimal and Seesaw using CaDiCaL~\autocite{BiereFazekasFleuryHeisinger-SAT-Competition-2020-solvers} as the SAT solver and CPLEX v20.10 for hitting set extraction.
For ParetoMCS, we use the publicly-available Sat4j-based~\autocite{DBLP:journals/jsat/BerreP10} implementation ({\small\url{https://gitlab.ow2.org/sat4j/moco}}).

% Tasks solved
\algname{} can be used to enumerate both, a representative solution for each Pareto point, and the full Pareto front.
With the extension presented in \cref{sec:sat-based}, $P$-minimal can solve these same two tasks.
For Seesaw, as mentioned before, extending it to enumerating the full Pareto front seems non-trivial, and for ParetoMCS, the given implementation only returns a single solution per Pareto point.
We therefore only compare to Seesaw and ParetoMCS on the task of enumerating a single representative solution per Pareto point.

% ParetoMCS variants
The implementation of ParetoMCS allows for optionally enabling stratification~\autocite{DBLP:conf/ijcai/Terra-NevesLM18}.
We use stratification for a second variant of ParetoMCS, which we refer to as ParetoMCS-strat.

% Seesaw instantiation for MLIC
Since Seesaw needs to be instantiated for each task separately, we instantiated Seesaw for learning interpretable decision rules by using misclassifications as the objective over which optimization cores are extracted and a hitting set $\hs$ is found over these optimization cores.
In the second step, the number of literals in the smallest rule misclassifying the samples in $\hs$ (or a subset of it) is found.
This function is implemented as a solution-improving search with a SAT solver.
This instantiation was chosen because finding the smallest rule misclassifying $\hs$ is an anti-monotone function and the refined version of core extraction presented in the original paper~\autocite{DBLP:conf/cp/JanotaMSM21} can therefore be used, making Seesaw feasible in the first place.
Furthermore, we instantiate two more variants of Seesaw where optimization cores are extracted in the solution-improving search procedure by the SAT solver.
The last SAT query of solution-improving search will always be \unsat{};
the core returned by the solver for this call can be used as an optimization core for Seesaw.
Optionally we apply exact core minimization to these cores before using them in Seesaw.
(We found that heuristic core minimization leads to no significant improvement over no minimization at all.)
These two variants will be referred to as Seesaw-SAT and Seesaw-SAT-min.

\section{Results\label{sec:results}}

% Signposting
We turn to an overview of the empirical results.
We first present a runtime comparison of the different approaches to bi-objective optimization for the task of finding a single representative solution per Pareto point.
Next, we show the same comparison for enumerating all Pareto-optimal solutions.
We conclude the results by an evaluation of the impact the refinements presented in \cref{sec:refinements} have on the performance of \algname{}.

\subsection{Finding a Single Representative Solution per Pareto Point}

% mlic-cactus-single
For comparing the performance of the variants of \algname{} presented in \cref{sec:variants}, $P$-minimal, ParetoMCS and Seesaw, we first discuss at the results for learning a single decision rule per Pareto point.
\Cref{fig:mlic-cactus-single} shows the number of instances solved (horizontal axis) for different per-instance time limits (vertical axis) for this task.
The best-performing approaches are the \algname{} variants \msh{}, \satunsat{}, \unsatsat{} and \msu{}, solving 223 instances, while $P$-minimal solved 219 instances.
All variants of \algname{} outperform $P$-minimal to some extent.
Seesaw and ParetoMCS are very clearly outperformed by all other approaches.
Even the best variant of Seesaw (Seesaw-SAT-min) only solves 136 instances within the resource constraints.
ParetoMCS performs even worse, solving only 34 instances without and 32 with stratification.

% mlic-cactus-single
\begin{figure}
  \centering
  \includegraphics{mlic-cactus-single.pdf}
  \caption{Runtime comparison between variants of \algname{} and competing approaches for learning interpretable decision rules;
    enumeration of a single representative solution per Pareto point.
    The plot on the right shows a magnification for comparing the best-performing approaches.
  }\label{fig:mlic-cactus-single}
\end{figure}

% setcover-cactus-single
Next, we consider a similar comparison for the bi-objective set covering benchmarks.
In this comparison, only the Seesaw variants with SAT core extraction are included since the default Seesaw core extraction strategy cannot be applied for this problem.
\Cref{fig:setcover-cactus-single} shows the number of solved instances given a per-instance time limit for the two generated sets of bi-objective set covering instances.
ParetoMCS with and without stratification, as well as Seesaw-SAT did not manage to solve any of the set covering instances.
The best-performing variant of \algname{} is again \msh{}, considerably outperforming $P$-minimal and Seesaw-SAT-min:
$P$-minimal solved 71 (respectively 38) fixed element probability (respectively set cardinality) instances, Seesaw-SAT-min solved 60 (respectively 38), whereas \msh{} solved 83 (respectively 40) instances.
For this application, not all variants of \algname{} outperformed $P$-minimal:
\msu{} and \oll{} were outperformed for both instance variants while \satunsat{} and \unsatsat{} were only outperformed for the instances generated with fixed set cardinality.
The good performance of Seesaw-SAT-min on these instances might be explained by the fact that the weights for one of the objectives in Seesaw are handled by an integer linear programming solver, rather than in propositional logic.

% setcover-cactus-single
\begin{figure}
  \centering
  \includegraphics{setcover-cactus-single.pdf}
  \caption{Runtime comparison between variants of \algname{} and competing approaches for bi-objective set covering problem;
    enumeration of a single representative solution per Pareto point.
  }\label{fig:setcover-cactus-single}
\end{figure}

% Overall solved instances and best-performaning
The numbers of solved instances for all approaches are summarized in \cref{tab:nsolved}.
The best-performing approach for each benchmark is highlighted in bold.
\msh{} is the best-performing \algname{} variant overall, outperforming $P$-minimal in all cases.
For more details, \cref{fig:combined-msh-single-obj-time} (left) shows a per-instance runtime comparison between \msh{} and $P$-minimal.
We note that $P$-minimal did not uniquely solve any instance.
In general, \msh{} was outperformed by $P$-minimal on only 31 instances while \msh{} solved 297 instances in less time.
Both, \algname{} and our implementation of $P$-minimal make full incremental use of the SAT solver, never resetting it during search.
This suggests that the advantage \algname{} has over $P$-minimal lies in the search being more structured.

% nsolved-single
\begin{table}
  \centering
  \caption{Solved instances by approach and benchmark domain.
    Results for both tasks, finding a single solution per Pareto point (single) and enumerating all Pareto-optimal solutions (all).
  }\label{tab:nsolved}
  \begin{tabular}{@{}l@{\hspace{2em}}rrrrrr@{}}
    \toprule
    & \multicolumn{2}{c}{Decision Rules} & \multicolumn{2}{c}{\scep{}} & \multicolumn{2}{c}{\scsc{}} \\
    Algorithm & single & all & single & all & single & all \\
    \midrule
    \satunsat{} & \textbf{223} & \textbf{215} & 77 & 75 & 35 & 35 \\
    \unsatsat{} & \textbf{223} & \textbf{215} & 71 & 71 & 29 & 29 \\
    \msu{} & \textbf{223} & \textbf{215} & 71 & 70 & 36 & 36 \\
    \oll{} & 222 & 213 & 58 & 58 & 34 & 34 \\
    \msh{} & \textbf{223} & \textbf{215} & \textbf{83} & \textbf{81} & \textbf{40} & \textbf{40} \\\addlinespace
    $P$-minimal & 219 & 213 & 71 & 68 & 38 & 26 \\\addlinespace
    Seesaw & 123 & -- & -- & -- & -- & -- \\
    Seesaw-SAT & 42 & -- & 0 & -- & 0 & -- \\
    Seesaw-SAT-min & 136 & -- & 60 & -- & 38 & -- \\\addlinespace
    ParetoMCS & 34 & -- & 0 & -- & 0 & -- \\
    ParetoMCS-strat & 32 & -- & 0 & -- & 0 & -- \\
    \bottomrule
  \end{tabular}
\end{table}

% combined-msh-single-obj-time
\begin{figure}
  \centering
  \includegraphics{all-datasets-combined-msh-single-obj-time.pdf}
  \caption{Left: Runtime comparison between $P$-minimal and \algname{} in the \msh{} variant.
    Right: Time (in seconds) spent in the \Min{} and \Simpr{} subroutines for \algname{} in the \msh{} variant.
    Both plots are for enumeration of a single representative per Pareto point.
  }\label{fig:combined-msh-single-obj-time}
\end{figure}

% Time spent per objective
\Cref{fig:combined-msh-single-obj-time} (right) shows how much time is spent in the \Min{} compared to the \Simpr{} subroutine for the \msh{} variant of \algname{}.
The arc that forms in this plot is the time limit border, which is where the sum of both axes equals the 5\,400-second per-instance time limit.
It can be seen that for the decision rule instances, in general more time is spent in the \Simpr{} subroutine, while for the set covering instances, more time is spent in the \Min{} subroutine.
This indicates that the answer to the question of which step of the lexicographic minimization is harder depends notably on the instance.
Furthermore, we note that the two objectives for the set covering instances are both generated randomly with the same procedure, therefore swapping the objectives does not change the instances.
For the decision rule instances, the objectives are structurally different, but---as mentioned previously---preliminary experiments have shown that this configuration of the objectives leads to better performance.

\subsection{Enumerating All Pareto-Optimal Solutions}

% mlic-cactus-multi
Next we compare the performance for learning all Pareto-optimal interpretable decision rules.
\Cref{fig:mlic-cactus-multi} shows the results for this task, presented in the same way as in \cref{fig:mlic-cactus-single}.
For this task, the \algname{} variants \satunsat{}, \unsatsat{}, \msu{} and \msh{} all performed the best, solving 215 instances each.
With this, each of them outperforms $P$-minimal, which solved 213 instances.
The \oll{} variant of \algname{} solved the same number of instances as $P$-minimal.
Note that Seesaw and ParetoMCS cannot be used for enumerating all Pareto-optimal solutions and are therefore excluded for these experiments.

% mlic-cactus-multi
\begin{figure}
  \centering
  \includegraphics{mlic-cactus.pdf}
  \caption{Runtime comparison between variants of \algname{} and competing approaches for learning interpretable decision rules;
    enumeration of all Pareto-optimal solutions.
    The plot on the right shows a magnification for comparing the best-performing approaches.
  }\label{fig:mlic-cactus-multi}
\end{figure}

% setcover-cactus-multi
Turning to the same comparison for bi-objective set covering, as seen from \cref{fig:setcover-cactus-multi}, \msh{} is the best-performing approach also here.
For the instances with fixed set cardinality, all \algname{} variants solved at least as many instances as $P$-minimal, for the instances with fixed element probability only \oll{} performed worse than $P$-minimal.
The best-performing variant, \msh{}, solved 81 (respectively 40) of the fixed element probability (respectively set cardinality) instances while $P$-minimal solved 68 (respectively 26).

% setcover-cactus-multi
\begin{figure}
  \centering
  \includegraphics{setcover-cactus.pdf}
  \caption{Runtime comparison between variants of \algname{} and competing approaches for bi-objective set covering problem;
    enumeration of all Pareto-optimal solutions.
  }\label{fig:setcover-cactus-multi}
\end{figure}

% Overall solved instances and best-performing
\Cref{tab:nsolved} summarizes the number of solved instances for all approaches applicable to this task.
It can be seen that \msh{} is the best-performing approach for this task as well.
A per-instance runtime comparison between \msh{} and $P$-minimal for the task of enumerating all Pareto-optimal solutions is shown in \cref{fig:combined-msh-scatter-single-multi} (left).
$P$-minimal did also not uniquely solve any instance for enumerating all Pareto-optimal solutions.
Furthermore, \msh{} was outperformed by $P$-minimal on 71 instances while \msh{} outperformed $P$-minimal on 257 instances.

% Single multi comparison
Comparing the number of solved instances in \cref{tab:nsolved}, we can see that the performance difference between \algname{} and $P$-minimal is greater when enumerating all Pareto-optimal solutions.
Furthermore, \cref{fig:combined-msh-scatter-single-multi} (right) shows a runtime comparison between enumerating a single representative solution per Pareto point and enumerating all Pareto-optimal solutions with \msh{}.
Overall, \algname{} scales well also for enumerating all Pareto-optimal solutions, although there is an overhead when the number of solutions required to be enumerated grows significantly;
this is the case for learning interpretable decision rules, where some instances have more than 10\,000 solutions per Pareto point.
This is in contrast to the set covering instances, which tend to have only a single (of few) solutions per Pareto point.
The fewer solutions per Pareto point for the set covering instances can be explained by the weighted objectives which make it significantly less likely that two distinct solutions have identical objective function values.

% combined-msh-scatter-single-multi
\begin{figure}
  \centering
  \includegraphics{all-datasets-combined-msh-and-enumeration.pdf}
  \caption{Left: Runtime comparison between $P$-minimal and \algname{} in the \msh{} variant; 
    enumeration of all Pareto-optimal solutions.
    Right: Runtime comparison between enumerating a single representative vs.\ all solutions per Pareto point with \msh{}.}\label{fig:combined-msh-scatter-single-multi}
\end{figure}

\subsection{Impact of Refinements\label{sec:res-ref}}

% Lazily building the totalizer for the decreasing objective
Finally, we evaluated the impact of the refinements proposed in \cref{sec:refinements} on the runtime efficiency of the best-performing approach, \msh{}.
As the first refinement considered, we evaluate the impact of lazily building the totalizer for the decreasing objective.
\Cref{fig:refinements-1} (left) shows a runtime comparison between \msh{} with and without lazy building of $\tot(\Obj_\dec)$.
It can be seen that for learning interpretable decision rules, this refinement has no evident impact.
This is to be expected since for these benchmarks, the literals from $\Obj_\dec$ do not appear in $\Obj_\inc$, so $\tot(\Obj_\dec)$ cannot be lazily built.
For the set covering instances with fixed element probability, the impact of this refinement is small and tends to be slightly negative for most instances.
However, for fixed set cardinality set covering, we see a strong positive effect.

% Core minimization
Next, we detail the impact of different core minimization strategies.
By default, \msh{} employs heuristic core minimization.
In \cref{fig:refinements-1} (right), the performance of this configuration is compared to using exact core minimization instead.
Heuristic core minimization appears to have a positive effect for the task of learning interpretable decision rules as well as for harder set covering instances.
However, the effect is smaller than that of lazily building $\tot(\Obj_\dec)$.

% refinements-1
\begin{figure}
    \centering
    \includegraphics{all-datasets-chosen-refinements-1.pdf}
    \caption{Instance runtime comparisons for the two refinements lazily building the totalizer for the decreasing objective (left) and exact core minimization (right).}\label{fig:refinements-1}
\end{figure}

% Blocking dominated solutions
\Cref{fig:refinements-2} (left) shows the effect of blocking dominated solutions in the \satunsat{} phase of \msh{}.
The impact of this refinement is negligible on all benchmarks, although, there are three instances of the set covering benchmarks with fixed element probability that were only solved when not blocking dominated solutions, giving this configuration a slight advantage.
On the decision rule instances, while slight positive effects of the refinement can be seen, the effect is not strong enough to enable solving additional instances.

% Disjoint phase
As the last refinement considered, we consider adding a disjoint core extraction phase to the \msu{} phase of \msh{}.
\Cref{fig:refinements-2} shows the impact of adding this refinement to the default configuration of \msh{}.
It can be seen that adding the disjoint phase does not result in a clear positive or negative effect.
However, the impact per instance varies a lot more compared to the impact of blocking dominated solutions.
The only clear negative impact can be seen on three instances for learning interpretable decision rules.
The strongest outlier is an instance that was solved by the configuration without a disjoint phase in under 500 seconds but only barely under the given time limit of 5\,400 seconds by the configuration with a disjoint phase.
At present, it is unclear to us why for this specific this refinement has such a strong negative effect.

% refinements-2
\begin{figure}
    \centering
    \includegraphics{all-datasets-chosen-refinements-2.pdf}
    \caption{Instance runtime comparisons for the two refinements blocking dominated solutions (left) and disjoint core extraction (right).}\label{fig:refinements-2}
\end{figure}