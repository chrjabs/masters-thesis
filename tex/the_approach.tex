\chapter{The \algname{} Algorithm\label{chap:approach}}

We detail the MaxSAT-based approach to bi-objective optimization developed in this work together with its variants.

\begin{algorithm}[t]
  \caption{\algname{}: MaxSAT-based  bi-objective optimization} %\TODO{Alternatively ``enumeration of'', but then it sounds like full enumeration again.}}
  \label{alg:base-algorithm}
  \textbf{Input}: A formula $\formula$, two objectives $\Obj_\inc$ and $\Obj_\dec$.\\
  \textbf{Output}: Either one or all pareto-optimal solution corresponding to each pareto point of $\formula$.

  \begin{algorithmic}[1]
    \STATE $\texttt{InitSATsolver}(F)$ \label{l:init-solv} 
    \STATE $(\res, \sol^r) \gets \satsolver(\emptyset)$ \quad\COMMENT{Invokes the SAT solver on the formula.}  \label{l:sols} 
    \IF{$\res=\text{UNSAT}$}
      \STATE \textbf{return} ``no solutions''
    \ENDIF
    \STATE $b_\dec \gets \infty, b_\inc \gets 0$ \label{l:bounds}
    \WHILE{$\res = \text{SAT}$} \label{l:loopstart}
      \STATE $(b_\inc, \sol^r) \gets \Min(b_\dec , \Obj_\inc(\sol^r))$  \quad\COMMENT{Maintains $\tot(\Obj_\inc)$ (or similar)}\label{l:minim1}
      \STATE $(b_\dec, \sol^r) \gets  \Simpr(b_\inc , \Obj_\dec(\sol^r))$  \quad\COMMENT{Builds $\tot(\Obj_\dec)$}\label{l:minim2}
      \STATE \textbf{yield} $\sol^r$  \quad\COMMENT{Optionally:  \textbf{yield} $\E(b_\dec, b_\inc)$} \label{ln:stage3} 
      \STATE $(\res, \sol^r) \gets \satsolver(\{\ov{\Obj_\dec}{b_\dec}\})$\label{l:endL}
    \ENDWHILE
  \end{algorithmic}
\end{algorithm}

\TODO{Make sure to differentiate between $\sol^r$ from the pseudocode and other $\sol$s.}

\section{Overview of the Algorithm\label{sec:algorithm}}

\Cref{alg:base-algorithm}, which we refer to as \algname{}, details the  framework for computing the pareto-optimal solutions of a CNF formula $\formula$ w.r.t.\ two objectives $\Obj_\inc$ and $\Obj_\dec$ that we focus on.
The framework is an instantiation of the general lexicographic optimization method~\autocite{survey} instantiated with a SAT solver.
More specifically, all subroutines of the procedure are implemented using a single instantiation of a SAT solver that is invoked incrementally and preserved (i.e., not reset) during the whole search. 
\algname{} maintains the bounds $b_\inc$ and $b_\dec$ on the two objectives $\Obj_\inc$ and $\Obj_\dec$, respectively.
In each iteration, the value of $b_\inc$ is set to the smallest value for which there is a still-undiscovered pareto-optimal solution $\tau$ for which $\Obj_\inc(\tau) = b_\inc$ by the $\Min$ procedure.
The value of $b_\dec$ is then set to $\Obj_\dec(\tau)$ by the $\Simpr$ procedure.
In case one wishes to enumerate all pareto-optimal solutions (in contrast to a single representative solution for each pareto point), the $\E$ procedure then enumerates all pareto-optimal solutions $\tau^o$ for which $\Obj_\inc(\tau^o) = b_\inc$ and $\Obj_\dec(\tau^o) = b_\dec$.

Importantly, since the value of $\Obj_\inc$ is always minimized first, the value $b_\inc$ returned each iteration is monotonically increasing. 
We therefore call $\Obj_\inc$ the \emph{increasing objective}.
By \cref{prop:biobjective}, this means that the sequence of values $b_\dec$ is monotonically decreasing, leading us to calling $\Obj_\dec$ \emph{decreasing}.
By these observations, \algname{} performs search in an ordered fashion along the pareto front.

In detail, given a formula $\formula$ and two objectives $\Obj_\inc$ and $\Obj_\dec$, the search of \algname{} in \cref{alg:base-algorithm} starts by initializing a SAT solver with all clauses in $\formula$ on \cref{l:init-solv}.
Satisfiability (i.e., the existence of any pareto-optimal solutions) is checked by invoking the SAT solver on its internal formula without assumptions via the $\satsolver(\emptyset)$ function (\cref{l:sols}).
If $\res$ is UNSAT, the formula has no pareto-optimal solutions and the algorithm terminates.
Otherwise, $\tau$ is an assignment that satisfies the formula.
Before the main enumeration procedure starts, the bounds $b_\inc$ and $b_\dec$ on $\Obj_\inc$ and $\Obj_\dec$ are set to $0$ and $\infty$, respectively.

The main search loop (\crefrange{l:loopstart}{l:endL}) iterates as long as there are pareto-optimal solutions of $\formula$ that have not been enumerated yet. 
This is the case if there is a solution $\tau$ for which $\Obj_\dec(\tau) < b_\dec$, checked by invoking the SAT solver under the assumption $\ov{\Obj_\dec}{b_\dec}$ on \cref{l:endL}.
In the beginning of each main loop iteration, the procedure $\Min$ is employed to minimize the increasing objective, i.e., compute the smallest value $b_\inc$ for which there is a solution $\tau$ for which $\Obj_\dec(\tau) < b_\dec$ and $\Obj_\inc(\tau) = b_\inc$  (\cref{l:minim1}). 
We assume that $\Min$ maintains a way to enforce that $\Obj_\inc(\tau) < k$, e.g., through a totalizer $\tot(\Obj_\inc)$, and that \algname{} and all of its subroutines have access to a set of assumptions to enforce this bound for any $k$.

Next, the algorithm employs \emph{solution-improving search}~\autocite{handbook2-maxsat,DBLP:journals/jsat/BerreP10,DBLP:journals/jsat/EenS06} to minimize the decreasing objective, i.e., to compute the smallest $b_\dec$ for which there is a solution $\tau$ for which $\Obj_\dec(\tau) = b_\dec$ and $\Obj_\inc(\tau) = b_\inc$  (\cref{l:minim2}).
The totalizer $\tot(\Obj_\dec, \Obj_\dec(\tau))$ is built at the first time this subroutine is invoked.
Building the totalizer at this point allows for only building it up to bound $\Obj_\dec(\tau)$, since all pareto-optimal solutions are known to have at most that value for $\Obj_\dec$.
Solution-improving search works by---starting from $k=\Obj_\dec(\tau)$---iteratively invoking the SAT solver under the assumptions $\{\ov{\Obj_\dec}{k}, \ove{\Obj_\inc}{b_\inc}\}$ for decreasing values of $k$ until the solver reports UNSAT, and returns $b_\dec=k+1$ and a solution $\tau$ for which $\Obj_\dec(\tau) = b_\dec$ and $\Obj_\inc(\tau) = b_\inc$.
At this point we know that there is no solution of $\formula$ that dominates $\tau$, so $\tau$ is returned as pareto-optimal on \cref{ln:stage3}.
If one wants to enumerate all solutions $\tau^o$ that correspond to the pareto point $(b_\inc,b_\dec)$, the $\E$ procedure repeatedly invokes the SAT solver with the assumptions $\{\ove{\Obj_\dec}{b_\dec} , \ove{\Obj_\inc}{b_\inc}\}$ and blocks each found solution until no more solutions are found.

% Main algorithm example
\begin{example}\label{ex:main-iteration}
  Invoke \algname{} on the formula $\formula$ and objectives $\Obj_\inc$, $\Obj_\dec$ detailed in \cref{fig:search-trace}. 
  The search starts by invoking a SAT solver on $\formula$.
  The call returns a feasible solution, say $\sol^c_1 = \solcone$ for which $\Obj_\inc(\sol^c_1) = \Obj_\dec(\sol^c_1) = 3$. 
  The first iteration of the main search loop starts with a call to \Min{}.
  This returns $b_\inc = 1$ and, e.g., the solution $\sol^c_3 = \solcthree$ for which $\Obj_\inc(\sol^c_3) = 1$ and $\Obj_\dec(\sol^c_3) = 3$.
  \algname{} then proceeds to the \Simpr{} subroutine that initializes a totalizer $\tot(\Obj_\dec, 3)$.
  The first call to the SAT solver is made with the assumptions $\assumps = \{ \ove{\Obj_\inc}{1}, \ov{\Obj_\dec}{4} \}$.
  The result is satisfiable;
  say that the solver returns the solution $\sol^o_1 = \soloone$.
  Then, the solver is invoked with the assumptions $\assumps =  \{ \ove{\Obj_\inc}{1}, \ov{\Obj_\dec}{3} \}$.
  The result is unsatisfiable, so the procedure returns the pareto-optimal $\sol^o_1$ and $b_\dec = \Obj_\dec(\sol^o_1) = 3$.
  Now optionally the procedure \E can be used to enumerate all other solutions corresponding to the pareto point $(\Obj_\inc(\sol^o_1),\Obj_\dec(\sol^o_1))$.
  At the end of the iteration, the SAT solver is queried with the assumption $\{ \ov{\Obj_\dec}{2} \}$.
  As the result is SAT and the solver returns, e.g., the solution $\sol^c_4 = \solcfour$,
  the algorithm starts a new iteration. \\
  The next iteration of \algname{} proceeds similarly to the first.
  The procedure \Min{} returns $b_\inc = 2$ and, e.g., the solution $\sol^o_2 = \solotwo$.
  \Simpr{} cannot improve on the decreasing objective, so the solution $\sol^o_2$ is proven to be pareto-optimal.
  At the end of the iteration, on \cref{l:endL} the SAT solver is invoked with the assumption $\{\ov{\Obj_\dec}{2}\}$ which returns SAT and, e.g., the solution $\sol^o_3 = \solothree$. \\
  The last iteration starts by calling \Min{} which returns $b_\inc = 3$ and, e.g., again the solution $\sol^o_3$.
  \Simpr{} again cannot improve on the decreasing objective, so $\sol^o_3$ is also pareto-optimal.
  Lastly, the SAT solver is queried with the assumption $\{\ov{\Obj_\dec}{1}\}$.
  The solver returns unsatisfiable, terminating the algorithm. 
\end{example}

\section{Approaches to Minimizing the Increasing Objective\label{sec:variants}}

We consider five different instantiations of the $\Min$ procedure for minimizing the increasing objective, inspired by MaxSAT algorithms. 

\subsection{\satunsat{}\label{sec:sat-unsat}}

\begin{algorithm}[t]
  \caption{\satunsat{} instantiation of \Min{}}\label{alg:sat-unsat}
  \textbf{Input}: Last bound $b_\dec$ on $\Obj_\dec$ and known satisfiable starting bound $k$ on $\Obj_\inc$ \\
  \textbf{Output}: The minimum value for $\Obj_\inc$ that can be achieved while the value for $\Obj_\dec$ is lower than $b_\dec$ and a solution fulfilling these conditions
  \TODO{Point out that returned $\sol^r$ is from last SAT call}

  \begin{algorithmic}[1]
    \STATE build or extend $\tot(\Obj_\inc,k)$ if necessary
    \STATE $(\res,\sol^r) \gets \satsolver(\{\ov{\Obj_\dec}{b_\dec}, \ov{\Obj_\inc}{k}\})$
    \WHILE{$\res = \text{SAT}$}
      \STATE $k \gets \Obj_\inc(\sol^r)$
      \STATE $(\res,\sol^r) \gets \satsolver(\{\ov{\Obj_\dec}{b_\dec}, \ov{\Obj_\inc}{k}\})$
    \ENDWHILE
    \STATE \textbf{return} $k,\sol^r$
  \end{algorithmic}
\end{algorithm}

\satunsat{} is a variant of solution-improving search that is also used for minimizing $\Obj_\dec$. 
The procedure gets as input the current bound $b_\dec$ on $\Obj_\dec$ and the value $\Obj_\inc(\tau)$ obtained by the solution $\tau$ computed during the last SAT solver call. 
Since the last call is made on \cref{l:endL} with the assumption $\ov{\Obj_\dec}{b_\dec}$, the solution $\tau$ will have $\Obj_\dec(\tau) < b_\dec$. 

As such, the value $\Obj_\inc(\tau)$ is an upper bound for the smallest value of $\Obj_\inc$ obtained by any solution $\tau'$ having $\Obj_\dec(\tau') < b_\dec$.
The procedure \satunsat{} maintains the totalizer $\tot(\Obj_\inc)$ and begins by checking, if the current upper bound on that totalizer is at least $\Obj_\inc(\tau)$, extending it if not. 
Then the SAT solver is iteratively invoked with the assumptions $\{\ov{\Obj_\dec}{b_\dec}, \ov{\Obj_\inc}{k}\}$ for decreasing values of $k$ starting from $\Obj_\inc(\tau)$.
The procedure terminates when the result is UNSAT, after which the value of $k$ and the solution obtained during the final satisfiable call are returned as $b_\inc$ and $\tau$.  

\begin{example}\label{ex:satunsat}
  Consider the invocation of \algname{} detailed in \cref{ex:main-iteration}. 
  We detail the invocation of \Min{} instantiated as \satunsat{}. 
  The full progression of the search of \algname{} with \Min{} instantiated as \satunsat{} is illustrated in \cref{fig:search-trace}.
  In the first iteration, \satunsat{} is invoked with $b_\dec =\infty$ and $\Obj_\inc(\sol^c_1) = 4$.
  At this point, the totalizer over $\Obj_\inc$ has not been built, so the procedure starts by adding $\tot(\Obj_\inc, 3)$ to the solver.
  The first call to the SAT solver is made with the assumptions $\{\ov{\Obj_\inc}{4}\}$, since $b_\dec  = \infty$ and therefore no assumption constraining $\Obj_\dec$ is needed.
  The result is satisfiable, the solver returns, e.g., the solution $\sol^c_2 = \solctwo$. 
  In the next iteration, the set of assumptions is $\{\ov{\Obj_\inc}{2}\}$.
  The result is again satisfiable, returning, e.g., the solution $\sol^c_3 = \solcthree$.
  The SAT solver is then invoked with the assumptions  $\{\ov{\Obj_\inc}{1}\}$.
  Now the result is UNSAT so the procedure terminates and returns $\sol^c_3$ and $b_\inc = 1$. \\
  In the second iteration of \algname{}, \satunsat{} is invoked with $b_\dec = 3$ and $\Obj_\inc(\sol^c_4) = 3$.
  The first call to the SAT solver is made with the assumptions $\{ \ov{\Obj_\dec}{3}, \ov{\Obj_\inc}{3}\}$.
  The result is SAT and the solver returns, e.g., the solution $\sol^o_2 = \solotwo$.
  \satunsat{} invokes the SAT solver again with the assumptions $\{ \ov{\Obj_\dec}{3}, \ov{\Obj_\inc}{2} \}$.
  The result is UNSAT, so the procedure returns $b_\inc = 2$ and $\sol^o_2$. \\
  In the thrid (and last) iteration of \algname{}, \satunsat{} is invoked with $b_\dec = 2$ and $\Obj_\inc(\sol^o_3) = 3$.
  The SAT solver is queried with the assumptions $\{ \ov{\Obj_\dec}{2}, \ov{\Obj_\inc}{3} \}$ and returns UNSAT.
  \satunsat{} therefore returns $\sol^o_3$ and $b_\inc = 3$.
\end{example}

\subsection{\unsatsat{}\label{sec:unsat-sat}}

\TODO{Define parameters of \Min{} (here $b_\inc$ for \satunsat{} $\Obj_\inc(\sol^r)$)}

\begin{algorithm}[t]
  \caption{\unsatsat{} instantiation of \Min{}}\label{alg:unsat-sat}
  \textbf{Input}: Last bound $b_\dec$ on $\Obj_\dec$ and last bound $b_\inc$ on $\Obj_\inc$ \\
  \textbf{Output}: The minimum value for $\Obj_\inc$ that can be achieved while the value for $\Obj_\dec$ is lower than $b_\dec$ and a solution fulfilling these conditions

  \begin{algorithmic}[1]
    \STATE $k \gets b_\inc$
    \STATE build/extend $\tot(\Act,k+1)$
    \STATE $(\res,\sol^r) \gets \satsolver(\{\ov{\Obj_\dec}{b_\dec}, \ove{\Obj_\inc}{k+1}\})$
    \WHILE{$\res = \text{UNSAT}$}
      \STATE $k \gets k+1$
      \STATE extend $\tot(\Obj_\inc,k+1)$
      \STATE $(\res,\sol^r) \gets \satsolver(\{\ov{\Obj_\dec}{b_\dec}, \ove{\Obj_\inc}{k+1}\})$
    \ENDWHILE
    \STATE \textbf{return} $k+1,\sol^r$
  \end{algorithmic}
\end{algorithm}

\unsatsat{} takes a similar approach to \satunsat{} search but searches for the smallest value by lower-bounding instead of upper-bounding.
It also maintains a totalizer $\tot(\Obj_\inc)$.
For finding the next solution, the bound $k$ is set to the last known value of $b_\inc$ and the solver is then iteratively queried for a new solution under the assumptions $\{ \ove{\Obj_\inc}{k+1}, \ov{\Obj_\dec}{b_\dec} \}$.
If the solver returns unsatisfiable, the bound $k$ is increased by $1$ and the solver is queried again.
The search ends once the solver returns satisfiable and in this case, the solution, and the bound are returned.
Since the bound of this lower bounding search procedure will only monotonically increase, it is enough if the totalizer $\tot(\Obj_\inc)$ is at every step built up to the bound $k+1$ and extended to the next bound in the next iteration.
This way, the SAT solver is always loaded with a minimum number of clauses.

\begin{example}
  Consider the invocation of \algname{} detailed in \cref{ex:main-iteration}. 
  Here we detail the invocation of \Min{} instantiated as \unsatsat{}.
  In the first iteration, \unsatsat{} is invoked with $b_\dec =\infty$ and $\Obj_\inc(\sol^c_1) = 4$.
  At this point, the totalizer over $\Obj_\inc$ has not been built, so the procedure starts by initializing $\tot(\Obj_\inc, 1)$ and invokes the SAT solver with the assumptions $\{\ove{\Obj_\inc}{0}\}$.
  The result is UNSAT, so the totalizer is extended to $\tot(\Obj_\inc, 2)$ and the SAT solver invoked with the assumptions $\{ \ove{\Obj_\inc}{1}\}$.
  The result is SAT and the procedure returns, e.g., $\sol^c_3 = \solcthree$ and $b_\inc = 1$. \\
  In the second iteration of \algname{}, \unsatsat{} is invoked with $b_\dec = 3$ and $\Obj_\inc(\sol^c_4) = 3$.
  The totalizer is extended to $\tot(\Obj_\inc, 3)$ and the solver is invoked with the assumptions $\{\ove{\Obj_\inc}{2}, \ov{\Obj_\dec}{3}\}$, starting from the previous bound $b_\inc$.
  The result is SAT, and the solver returns, e.g., $\sol^o_2 = \solotwo$ and $b_\inc = 2$. \\
  In the last iteration of \algname{}, \unsatsat{} is invoked with $b_\dec = 2$ and $\Obj_\inc(\sol^o_3) = 3$.
  Since $\Obj_\inc(\sol^o_3)$ is already only 1 larger than the previous bound, the solver doesn't need to be queried and \unsatsat{} can directly return $b_\inc = 3$ and $\sol^o_3$.
\end{example}

\subsection{\msu{}\label{sec:msu3}}

\begin{algorithm}[t]
  \caption{\msu{} instantiation of \Min{}}\label{alg:msu3}
  \textbf{Input}: Last bound $b_\dec$ on $\Obj_\dec$ and last bound $b_\inc$ on $\Obj_\inc$ \\
  \textbf{Output}: The minimum value for $\Obj_\inc$ that can be achieved while the value for $\Obj_\dec$ is lower than $b_\dec$ and a solution fulfilling these conditions

  \begin{algorithmic}[1]
    \STATE $k \gets b_\inc$
    \STATE $(\res,\sol^r,\core) \gets \satsolver(\{\ov{\Obj_\dec}{b_\dec}, \ove{\Obj_\inc}{k+1}\} \cup \{\lnot l \mid l \in \Obj_\inc \setminus \Act\})$
    \WHILE{$\res = \text{UNSAT}$}
      \STATE $k \gets k+1$
      \STATE $\core \gets \core \setminus \{\lnot\ov{\Obj_\dec}{b_\dec}, \lnot\ove{\Obj_\inc}{k+1}\}$
      \STATE $\Act \gets \Act \cup \core$
      \STATE extend $\tot(\Act,k)$
      \STATE $(\res,\sol^r,\core) \gets \satsolver(\{\ov{\Obj_\dec}{b_\dec}, \ove{\Obj_\inc}{k+1}\} \cup \{\lnot l \mid l \in \Obj_\inc \setminus \Act\})$
    \ENDWHILE
    \STATE \textbf{return} $k,\sol^r$
  \end{algorithmic}
\end{algorithm}

\msu{} implements a core-guided approach~\autocite{DBLP:journals/corr/abs-0712-1097,DBLP:conf/sat/AnsoteguiBL09}, maintaining a set $\Act \subset \Obj_\inc$ of \emph{active} objective literals and a totalizer $\tot(\Act)$ built over them. 
Initially, $\Act = \emptyset$, i.e., all literals of $\Obj_\inc$ are inactive.
Informally speaking, an inactive literal $l \in \Obj_\inc \setminus \Act$ is assumed to the value $0$ in every invocation of the SAT solver until it is returned as part of a core.
More precisely, on input $b_\dec$ and $\Obj_\inc(\tau)$, the algorithm starts from the value $b_\inc$ computed in the previous iteration and invokes the SAT solver with the assumptions $\assumps = \{\ove{\Act}{b_\inc}, \ov{\Obj_\dec}{b_\dec}  \} \cup \{ \lnot l \mid l \in \Obj_\inc \setminus \Act\}$.
If the result is unsatisfiable, the SAT solver returns a core $\core \subset \{\lnot l \mid l \in \assumps\}$.
Next, the bound $b_\inc$ is increased by one, the inactive literals in $\core$ are added to $\Act$ and the totalizer $\tot(\Act)$ is extended.
The procedure continues until the SAT solver returns satisfiable, and a solution $\tau$ which sets $\Obj_\inc(\tau) \leq b_\inc$ and $\Obj_\dec(\tau) < b_\dec$.
At that point the value $b_\inc$ is the minimum value of $\Obj_\inc(\tau)$ subject to $\Obj_\dec(\tau) < b_\dec$.
This is because the value of $b_\inc$ is increased monotonically, and the solver returned unsatisfiable in the second-to-last iteration. 

For enforcing $\ove{\Obj_\inc}{k}$ when employing $\msu$, consider an invocation of $\msu(b_\dec , \Obj_\inc(\tau))$ made during $\algname$ and assume it returns the tuple $(b_\inc, \tau)$. 
In the next call to $\Simpr$, the number of literals in $\Obj_\inc$ set to $1$ needs to be restricted to at most $b_{\inc}$. 
Since the totalizer maintained by $\msu$ only has $\Act \subset \Obj_{\inc}$ as inputs, we do not have access to an output literal of form  $\ove{\Obj_\inc}{b_{\inc}}$.
Instead, we use  the assumptions $\{\ove{\Act}{b_{\inc}}\} \cup \{ \lnot l \mid l \in \Obj_\inc \setminus \Act \}$, i.e., restrict the number of literals in $\Act$ set to $1$ to $b_{\inc}$ and assume the value of each inactive literal $l \in \Obj_{\inc} \setminus \Act$ to $0$. 
The following observation proves that doing so can be done without removing any pareto-optimal solutions from the search. 
\begin{observation}\label{obs:sound}
  Let $\tau$ be a pareto-optimal solution of $\formula$ for which $\Obj_{\inc}(\tau) = b_{\inc}$.
  Then $\tau(l) = 0$ for all $l \in \Obj_{\inc} \setminus \Act$. 
\end{observation}
\begin{proof}(Sketch)
  Since, $b_{\inc}$ was returned by $\msu$, we know that there exists a pareto-optimal $\tau^o$ for which $\Obj_{\inc}(\tau^o) = b_{\inc}$ and $\Obj_{\dec}(\tau^o) < b_{\dec}$.
  By the properties of cores, we also know that \emph{any} solution $\tau^s$ of $\formula$ for which $\Obj_{\dec}(\tau^s) < b_\dec$ assigns at least $b_{\inc}$ literals in $\Act$ to $1$.
  Thus, any $\tau^n$ that assigns $\tau^n(l) = 1$ for an inactive literal $l \in  \Obj_{\inc} \setminus \Act$ will have $\Obj_{\inc}(\tau^n) > b_{\inc}$.
\end{proof}

%Note that whenever a bound on $\Obj_\inc$ is enforced in the other subroutines of \algname{} by an assumption of type $\ove{\Obj_\inc}{k}$, for \msu{}, this assumption needs to be replaced by $\{\ove{\Act}{k}\} \cup \{ \lnot l \mid l \in \Obj_\inc \setminus \Act \}$.

\begin{example}\label{ex:msu}
  Consider the invocation of \algname{} detailed in \cref{ex:main-iteration}. 
  Here we detail the invocations of \Min{} instantiated as \msu{}. 
  In the first iteration of \algname{}, \msu{} is invoked with $b_\dec =\infty$ and $\Obj_\inc(\sol^c_1) = 4$.
  Initially, the set $\Act = \emptyset$ of active literals is empty, so the first call to the SAT solver is made with the assumptions $\assumps =  \{ \lnot i_1, \lnot i_2, \lnot i_3, \lnot i_4\}$.
  The result is unsatisfiable and the solver returns, e.g., $\core = \{i_1, i_2\}$. 
  The literals in $\core$ are marked as active and the totalizer $\tot(\Act, 2)$ is initialized.
  The SAT solver is then invoked with the assumptions $\assumps = \{ \lnot i_3, \lnot i_4, \ove{\Act}{1}\}$. 
  The result is satisfiable so the solver returns, e.g., the solution $\sol^c_3 = \solcthree$ and $b_\inc = 1$. \\
  In the next iteration of \algname{}, \msu{} is invoked with $b_\dec = 3$ and $\Obj_\inc(\sol^c_4) = 3$.
  The set $\Act = \{i_1, i_2\}$ is kept from the previous iterations, so the first call to the SAT solver is made with the assumptions  $\assumps = \{ \lnot i_3, \lnot i_4, \ove{\Act}{1},  \ov{\Obj_\dec}{3} \}$.
  The result is unsatisfiable;
  assume the core returned by the solver is $\core = \{i_3, i_4, \lnot \ove{\Act}{1}, \lnot \ov{\Obj_\dec}{3}\}$.
  The totalizer outputs $\lnot \ove{\Act}{1}$ and $\lnot \ov{\Obj_\dec}{3}$ are discarded, $i_3$ and $i_4$ are added to the active literals, and the totalizer is extended to $\tot(\Act, 3)$.
  The SAT solver is queried again with the assumptions $\assumps = \{\ove{\Act}{2}, \ov{\Obj_\dec}{2}\}$;
  the result is SAT and the returned solution, e.g., $\sol^o_2 = \solotwo$.
  \msu{} returns $b_\inc = 2$ and $\sol^o_2$. \\
  In the last iteration, \msu{} is invoked with $b_\dec = 2$ and $\Obj_\inc(\sol^o_3) = 3$.
  The SAT solver is queried with the assumptions $\assumps = \{\ove{\Act}{2}, \ov{\Obj_\dec}{2}\}$.
  The result is UNSAT;
  assume the core is $\core = \{ \lnot \ove{\Act}{2}, \lnot \ov{\Obj_\dec}{2} \}$.
  Both totalizer outputs are discarded, and the totalizer is extended to $\tot(\Act, 4)$.
  The solver is queried again with the assumptions $\assumps = \{\ove{\Act}{3}, \ov{\Obj_\dec}{2}\}$.
  The result is SAT;
  assume the returned solution is $\sol^o_3 = \solothree$.
  \msu{} returns $b_\inc = 3$ and $\sol^o_3$.
\end{example}

\subsection{\oll{}\label{sec:oll}}

\oll{}, as another core-guided procedure (originally proposed in the context of ASP~\autocite{DBLP:conf/iclp/AndresKMS12} and also successfully applied in MaxSAT~\autocite{DBLP:conf/cp/MorgadoDM14,DBLP:journals/jsat/IgnatievMM19}), handles the cardinality constraint over the literals in $\Obj_\inc$ differently to \msu{}.
Instead of a single totalizer over all literals in $\Act$, a separate totalizer is built for every core returned after the unsatisfiable SAT solver calls.
In each iteration, the assumptions given to the SAT solver consist of (i)~the inactive literals of $\Obj_\inc$, (ii)~the outputs of previously built totalizers corresponding to the lowest number of input literals that should be assigned to $1$ in any possible satisfying assignment and (iii)~the bound $\ov{\Obj_\dec}{b_\dec}$.
The procedure terminates when the SAT solver returns a solution $\tau$.
Similarly to $\msu$, the assumptions for enforcing a bound on $\Obj_\inc$ in the other subroutines of \cref{alg:base-algorithm} need to be adapted when using $\oll$.
%by assuming the inactive literals of $l \in \Obj_{\inc} \setminus \Act$ to $0$. \TODO{Even more adaption is needed since the outputs of all the totalizers need to be assumed.}

\subsection{\msh{}\label{sec:hybrid}}

\begin{algorithm}[t]
  \caption{\msh{} instantiation of \Min{}}\label{alg:msh}
  \textbf{Input}: Last bound $b_\dec$ on $\Obj_\dec$, known satisfiable starting bound $k$ on $\Obj_\inc$, last bound $b_\inc$ on $\Obj_\inc$ and threshold value $0<\thr\le 1$ for when to switch to \satunsat{} \\
  \textbf{Output}: The minimum value for $\Obj_\inc$ that can be achieved while the value for $\Obj_\dec$ is lower than $b_\dec$ and a solution fulfilling these conditions

  \begin{algorithmic}[1]
    \IF{$|\Act| < \thr \cdot |\Obj_\inc|$}
      \STATE $b_\inc,\tau^r \gets \msu{}(b_\dec, b_\inc)$ \COMMENT{Immediately terminates once $|\Act| < \thr \cdot |\Obj_\inc|$ is reached}
    \ENDIF
    \IF{$|\Act| \ge \thr \cdot |\Obj_\inc|$}
      \STATE $b_\inc,\tau^r \gets \satunsat{}(b_\dec, k)$
    \ENDIF
    \STATE \textbf{return} $b_\inc,\tau^r$
  \end{algorithmic}
\end{algorithm}

The final variant proposed in this work, \msh{}, is a hybrid between \msu{} and \satunsat{} with the following intuition:
if \msu{}  reaches the stage where all literals of the objective are active, its search will become equivalent to \unsatsat{}.
%, meaning it is a lower bounding search where the bound on the totalizer $\tot(\Obj_\inc)$ is increased by one every iteration until the SAT query is satisfiable.
However, \satunsat{} search may be a significantly better approach compared to \unsatsat{}.
If this is the case, \msu{} might have an advantage over \satunsat{} as long as not all literals are active, but as soon as all literals are active, it looses its advantage.
Furthermore, if a problem instance has literals in $\Obj_\inc$ that are not constrained by $\formula$, these literals will never appear in any core making \msu{} behave like \unsatsat{} even before the totalizer is fully built.

With this intuition, we propose \msh{} as a hybrid variant that starts with \msu{} search and switches over to \satunsat{} as soon as a certain percentage of the literals in $\Obj_\inc$ have been added to the totalizer $\tot(\Act)$.
Before switching over to \satunsat{}, the remaining literals are added to the totalizer to build $\tot(\Obj_\inc)$, which is needed for \satunsat{}.
With this, the advantages of both \msu{} and \satunsat{} can in the best case be combined.
\TODO{Expand.}

\begin{example}
  Consider the invocation of \algname{} detailed in \cref{ex:main-iteration}.
  We detail the invocations of \Min{} instantiated as \msh{}.
  Since \msh{} starts out as \msu{}, the first invocation follows the description in \cref{ex:msu}. \\
  Assume \msh{} is configured to switch as soon as 70\% of the literals in $\Obj_\inc$ are active.
  Since after the first iteration of \algname{} we have $\Act = \{i_1, i_2\}$, the second invocation of \msh{} also starts as \msu{} since less than 70\% of the literals in $\Obj_\inc$ are active.
  As soon as $i_3$ and $i_4$ become active, with the first core in the second invocation of \msu{}, the \msu{} subroutine is terminated since the threshold for switching to \satunsat{} is reached.
  Because all literals in $\Obj_\inc$ are already active in this example and therefore included in $\tot(\Act)$, the totalizer does not need to be extended but \satunsat{} can directly be invoked as in the second iteration outlined in \cref{ex:satunsat}. \\
  In the third iteration of \algname{}, \msh{} will directly invoke \satunsat{}, which proceeds as described in the third iteration in \cref{ex:satunsat}.
\end{example}

\section{Refinements to \algname{}\label{sec:refinements}}

\subsection{Lazily Building $\tot(\Obj_\dec)$}

Assume \algname{} is invoked on a formula $\formula$ and a pair of overlapping objectives $\Obj_\inc$ and $\Obj_\dec$ for which $\Obj_\inc \cap \Obj_\dec \neq \emptyset$ with $\Min$ instantiated as $\msu$ or $\oll$. Let $\Act$ be the set of active literals of $\Obj_\inc$ as maintained by $\Min$.
Lazy building of $\tot(\Obj_\dec)$ refers to only having $(\Obj_\dec \setminus \Obj_\inc) \cup  (\Act \cap \Obj_\dec)$ as input to the totalizer (incrementally extending the totalizer as the set $\Act$ grows), and assuming the value of each literal $l \in (\Obj_\dec \cap \Obj_\inc) \setminus \Act$ to $0$ in each SAT call made during invocations of $\Simpr$.
The soundness of doing so follows by an argument very similar to the one we made in \cref{obs:sound}.
\TODO{elaborate}
%Essentially, the properties of cores imply that the pareto optimal solutions $\tau$ of $\formula$ for which $\Obj_{\inc}(\tau) = b_\inc$ assign $\tau(l) = 0$ for all  $l \in (\Obj_\dec \cap \Obj_\inc) \setminus \Act$ to $1$. 

Lazy building of $\tot(\Obj_\dec)$ requires a minor adaption to the termination criterion of \cref{alg:base-algorithm}.
More specifically, as the totalizer maintained by $\Simpr$ might not have all literals of $\Obj_\dec$ as inputs, the algorithm does not have a (straight-forward) way of checking if there exists a solution $\tau$ for which $\Obj_\dec(\tau) < b_\dec$.
However, the lack of further pareto-optimal solutions is instead detected in the next call to $\Min$ by the SAT solver returning an empty core, or more precisely, a subset of assumptions that doesn't contain any inactive literals nor outputs of $\tot(\Obj_\inc)$.

\subsection{Blocking of Dominated Solutions}

Every time in the search procedure that a candidate solution $\tau_c$ with objective values $b_\inc = \Obj_\inc(\tau_c)$ and $b_\dec = \Obj_\dec(\tau_c)$ is found, the definition of a pareto-optimal point leads to the conclusion that all solutions $\tau_d$ with $\Obj_\inc(\tau_d) > b_\inc$ and $\Obj_\dec(\tau_d) > b_\dec$ cannot be pareto-optimal.
These points can all be blocked by adding the clause $\{ \ove{\Obj_\inc}{b_\inc}, \ove{\Obj_\dec}{b_\dec} \}$ to the solver.
\TODO{elaborate}

\subsection{Domain-Specific Solution Blocking}

If multiple representatives of the same pareto point are of interest, the procedure $\E$ needs to block all obtained solutions. 
While this can be done in a straight-forward manner on the CNF-level, we will in later sections give examples of how domain-specific knowledge can be used in order to derive stronger clauses that block not only a specific solution obtained, but also other, symmetric solutions.

\subsection{Bound Hardening}

\TODO{write}

\subsection{Refinements to Core-Guided Variants}

Our implementation of the variants $\algname$ with $\msu$ or $\oll$ make use of refinements commonly used in core-guided MaxSAT solving.
More specifically, we employ core minimization~\autocite{DBLP:journals/jsat/IgnatievMM19} (either exact or heuristic) and core-exhaustion~\autocite{DBLP:journals/jsat/IgnatievMM19,DBLP:conf/cp/AnsoteguiBGL13}.
%(We also considered and implemented a disjoint core phase~\cite{DBLP:conf/cp/DaviesB11} but did not observe noticeable impact on runtime performance in practice.)
Given a core $\core$ returned by the SAT solver, heuristic core minimization refers to reinvoking the SAT solver with $\{\lnot l \mid l \in \core\}$ as the assumptions hoping that the solver returns a smaller set of assumptions.
Exact core minimization refers to iteratively finding a minimal unsatisfiable subset by attempting to remove each assumption separately.
Core exhaustion is an OLL-specific technique that seeks to improve the lower bound of each totalizer being added.
%A disjoint core phase 
%refers to iteratively invoking the SAT solver in order to extract several disjoint sets of objective literals to add to the totalizer (when using $\msu$) or build new totalizers over (when using $\oll$).